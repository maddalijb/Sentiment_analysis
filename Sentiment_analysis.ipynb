{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#read the dataset and store it in a variable\n#describe and print the head of data \ndata=pd.read_csv('../input/tweets-sentiment-classification/train_2kmZucJ.csv')\nprint(data.head)\ndata.describe()\ntype(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preproces the data(data-cleaning)\n#remove all the duplicates and remove all the null values\n#print the shape of the dataset\ndata.dropna()\ndata.drop_duplicates()\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print columns of dataset\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets=data['tweet'].values\ny=data['label'].values\ntweets,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos=data[data['label']==0]\nneg=data[data['label']==1]\nprint(pos.shape)\nprint(neg.shape)\npos.columns\ntype(pos)\npos.head()\nneg.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos.describe()\nneg.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nlabels='positive','negative'\ncolors=['green','red']\nsizes=[pos.shape[0],neg.shape[0]]\nplt.pie(sizes,labels=labels,colors=colors,autopct='%1.1f%%',shadow=True)\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pos=pos[:4000]\ntest_pos=pos[4000:]\ntrain_neg=neg[:1500]\ntest_neg=neg[1500:]\ntrain_x=pd.concat([train_pos,train_neg])\ntest_x=pd.concat([test_pos,test_neg])\n\ntrain_y = np.append(np.zeros(len(train_pos),dtype='int'), np.ones(len(train_neg),dtype='int'))\ntest_y = np.append(np.zeros(len(test_pos),dtype='int'), np.ones(len(test_neg),dtype='int'))\nprint(train_y.shape)\nprint(test_y.shape)\ntrain_x=train_x['tweet']\ntype(train_x)\ntrain_x=train_x.tolist()\ntest_x=test_x['tweet'].tolist()\ntype(train_x)\ntrain_x[:10]\ntype(train_y)\nprint(len(train_x))\nprint(len(train_y))\ntrain_y=train_y.tolist()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom nltk.tokenize import TweetTokenizer\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re                                  # library for regular expression operations\nimport string                              # for string operations\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_tweet(tweet):\n    p_tweet = re.sub(r'^RT[\\s]+', '', tweet)\n\n    # remove hyperlinks\n    p_tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', p_tweet)\n\n    # remove hashtags\n    # only removing the hash # sign from the word\n    p_tweet = re.sub(r'#', '', p_tweet)\n    #tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(p_tweet)\n    \n    #Import the english stop words list from NLTK\n    stopwords_english = stopwords.words('english') \n    \n    tweets_clean = []\n    for word in tweet_tokens: # Go through every word in your tokens list\n        if (word not in stopwords_english and word not in string.punctuation):  # remove punctuation\n            tweets_clean.append(word)\n    \n    stemmer = PorterStemmer() \n\n    # Create an empty list to store the stems\n    tweets_stem = [] \n\n    for word in tweets_clean:\n        stem_word = stemmer.stem(word)  # stemming word\n        tweets_stem.append(stem_word)  # append to the list\n    return tweets_stem\n    \n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_tweets(result, tweets, ys):\n    for y, tweet in zip(ys, tweets):\n        for word in process_tweet(tweet):\n            # define the key, which is the word and label tuple\n            pair = (word,y)\n\n            # if the key exists in the dictionary, increment the count\n            if pair in result:\n                result[pair] += 1\n\n            # else, if the key is new, add it to the dictionary and set the count to 1\n            else:\n                result[pair] = 1\n    ### END CODE HERE ###\n\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freqs=count_tweets({},train_x,train_y)\nfreqs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_naive_bayes(freqs, train_x, train_y):\n    loglikelihood = {}\n    logprior = 0\n    # calculate V, the number of unique words in the vocabulary\n    vocab = set([pair[0] for pair in freqs.keys()])\n    V = len(vocab)\n\n    # calculate N_pos and N_neg\n    N_pos = N_neg = V_pos =V_neg=0\n    for pair in freqs.keys():\n\n        if pair[1] == 1:\n           \n            V_neg+=1\n            \n            N_neg += freqs[pair]\n    \n        else:\n            V_pos+=1\n        \n            N_pos += freqs[pair]\n\n\n    # Calculate D, the number of documents\n    D = len(pos)+len(neg)\n\n    # Calculate D_pos, the number of positive documents\n    D_pos = len(pos)\n\n    # Calculate D_neg, the number of negative documents\n    D_neg = len(neg)\n\n    # Calculate logprior\n    logprior = np.log(D_pos)-np.log(D_neg)\n\n    # For each word in the vocabulary...\n    for word in vocab:\n        # get the positive and negative frequency of the word\n        freq_pos = freqs.get(word,0)\n        freq_neg = freqs.get(word,1)\n\n        # calculate the probability that each word is positive, and negative\n        p_w_pos = (freq_pos+1)/(N_pos+V)\n        p_w_neg = (freq_neg+1)/(N_neg+V)\n\n        # calculate the log likelihood of the word\n        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n\n    return logprior, loglikelihood\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\nprint(logprior)\nprint(len(loglikelihood))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def naive_bayes_predict(tweet, logprior, loglikelihood):\n    '''\n    Input:\n        tweet: a string\n        logprior: a number\n        loglikelihood: a dictionary of words mapping to numbers\n    Output:\n        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n\n    '''\n    # process the tweet to get a list of words\n    word_l = process_tweet(tweet)\n\n    # initialize probability to zero\n    p = 0\n\n    # add the logprior\n    p +=logprior\n\n    for word in word_l:\n\n        # check if the word exists in the loglikelihood dictionary\n        if word in loglikelihood:\n            # add the log likelihood of that word to the probability\n            p += loglikelihood[word]\n\n    return p\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n    accuracy = 0\n    y_hats = []\n    for tweet in test_x:\n        # if the prediction is > 0\n        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n            # the predicted class is 1\n            y_hat_i = 1\n        else:\n            # otherwise the predicted class is 0\n            y_hat_i =0\n\n        # append the predicted class to the list y_hats\n        y_hats.append(y_hat_i)\n\n    # error is the average of the absolute values of the differences between y_hats and test_y\n    error = np.mean(np.abs(y_hats-test_y))\n\n    # Accuracy is 1 minus the error\n    accuracy = 1-error\n\n    ### END CODE HERE ###\n\n    return accuracy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Naive Bayes accuracy = %0.4f\" %\n      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}